{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this project we will implement machine translation system using pre-trained embeddings."
      ],
      "metadata": {
        "id": "9GuVeMWB1zDy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Libraries"
      ],
      "metadata": {
        "id": "h31VnUFg6kra"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBga7-CF1pAv",
        "outputId": "97bda87b-fe85-4116-9355-6bdfa22aa7fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pdb\n",
        "import pickle\n",
        "import string\n",
        "import time\n",
        "import gensim\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import scipy\n",
        "import sklearn\n",
        "import os\n",
        "from gensim.models import KeyedVectors\n",
        "from nltk.corpus import stopwords, twitter_samples\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from os import getcwd\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('twitter_samples')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processing Tweet"
      ],
      "metadata": {
        "id": "5a-OTffk6n-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function tokenizes the tweet into individual words, removes stop words and applies stemming.\n",
        "def process_tweet(tweet):\n",
        "    \n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    # remove stock market tickers like $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    # remove hyperlinks\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    # tokenize tweets\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if (word not in stopwords_english and  # remove stopwords\n",
        "            word not in string.punctuation):  # remove punctuation\n",
        "            # tweets_clean.append(word)\n",
        "            stem_word = stemmer.stem(word)  # stemming word\n",
        "            tweets_clean.append(stem_word)\n",
        "\n",
        "    return tweets_clean"
      ],
      "metadata": {
        "id": "3sj7omF67mT3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining dictionary to convert from English to French"
      ],
      "metadata": {
        "id": "pv3Eh9Ky644r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function returns the english to french dictionary given a file where the each column corresponds to a word.\n",
        "def get_dict(file_name):\n",
        "    \n",
        "    my_file = pd.read_csv(file_name, delimiter=' ')\n",
        "    etof = {}  # the english to french dictionary to be returned\n",
        "    for i in range(len(my_file)):\n",
        "        # indexing into the rows.\n",
        "        en = my_file.loc[i][0]\n",
        "        fr = my_file.loc[i][1]\n",
        "        etof[en] = fr\n",
        "\n",
        "    return etof"
      ],
      "metadata": {
        "id": "M0DiHzMT7vZf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computing Similarity between two vectors"
      ],
      "metadata": {
        "id": "ypujeDR27Qoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(A, B):\n",
        "    \n",
        "    dot = np.dot(A, B)\n",
        "    norma = np.linalg.norm(A)\n",
        "    normb = np.linalg.norm(B)\n",
        "    cos = dot / (norma * normb)\n",
        "\n",
        "    return cos"
      ],
      "metadata": {
        "id": "hpZSHnD_2l1Z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading word Embeddings and English, French vocab"
      ],
      "metadata": {
        "id": "U0nM2gtG7dFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! git clone https://github.com/MarwanMohamed95/Machine-Translation-System\n",
        "\n",
        "# loading embedding\n",
        "en_embeddings_subset = pickle.load(open(\"/content/Machine-Translation-System/en_embeddings.p\", \"rb\"))\n",
        "fr_embeddings_subset = pickle.load(open(\"/content/Machine-Translation-System/fr_embeddings.p\", \"rb\"))\n",
        "# loading the english to french dictionaries\n",
        "en_fr_train = get_dict('/content/Machine-Translation-System/en-fr.train.txt')\n",
        "print('The length of the English to French training dictionary is', len(en_fr_train))\n",
        "en_fr_test = get_dict('/content/Machine-Translation-System/en-fr.test.txt')\n",
        "print('The length of the English to French test dictionary is', len(en_fr_train))"
      ],
      "metadata": {
        "id": "JV2KJFcXI8Wu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### building English and French matrices"
      ],
      "metadata": {
        "id": "zxeb4CUN7tGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_matrices(en_fr, french_vecs, english_vecs):\n",
        "\n",
        "    # X_l and Y_l are lists of the english and french word embeddings\n",
        "    X_l = list()\n",
        "    Y_l = list()\n",
        "    \n",
        "    # get the english words (the keys in the dictionary) and store in a set()\n",
        "    english_set = english_vecs.keys()\n",
        "\n",
        "    # get the french words (keys in the dictionary) and store in a set()\n",
        "    french_set = french_vecs.keys()\n",
        "\n",
        "    # store the french words that are part of the english-french dictionary (these are the values of the dictionary)\n",
        "    french_words = set(en_fr.values())\n",
        "\n",
        "    # loop through all english, french word pairs in the english french dictionary\n",
        "    for en_word, fr_word in en_fr.items():\n",
        "\n",
        "        # check that the french word has an embedding and that the english word has an embedding\n",
        "        if fr_word in french_set and en_word in english_set:\n",
        "\n",
        "            # get the english embedding\n",
        "            en_vec = english_vecs[en_word]\n",
        "\n",
        "            # get the french embedding\n",
        "            fr_vec = french_vecs[fr_word]\n",
        "\n",
        "            # add the english embedding to the list\n",
        "            X_l.append(en_vec)\n",
        "\n",
        "            # add the french embedding to the list\n",
        "            Y_l.append(fr_vec)\n",
        "\n",
        "    # stack the vectors of X_l into a matrix X\n",
        "    X = np.vstack(X_l)\n",
        "\n",
        "    # stack the vectors of Y_l into a matrix Y\n",
        "    Y = np.vstack(Y_l)\n",
        "    \n",
        "    return X, Y"
      ],
      "metadata": {
        "id": "PPCO0Hj9NPp6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the training set:\n",
        "X_train, Y_train = get_matrices(en_fr_train, fr_embeddings_subset, en_embeddings_subset)"
      ],
      "metadata": {
        "id": "BSlqvy98OU2q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss function"
      ],
      "metadata": {
        "id": "v76bAmCB7-Ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(X, Y, R):\n",
        "    \n",
        "    # m is the number of rows in X\n",
        "    m = X.shape[0]\n",
        "    \n",
        "    # diff is XR - Y\n",
        "    diff = np.dot(X,R)-Y\n",
        "\n",
        "    # diff_squared is the element-wise square of the difference\n",
        "    diff_squared = diff**2\n",
        "\n",
        "    # sum_diff_squared is the sum of the squared elements\n",
        "    sum_diff_squared = np.sum(diff_squared)\n",
        "\n",
        "    # loss i the sum_diff_squard divided by the number of examples (m)\n",
        "    loss = sum_diff_squared/m\n",
        "    return loss"
      ],
      "metadata": {
        "id": "CSCT-wW6OqOP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient function"
      ],
      "metadata": {
        "id": "2aSlKdGz8CyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient(X, Y, R):\n",
        "    \n",
        "    # m is the number of rows in X\n",
        "    m = X.shape[0]\n",
        "\n",
        "    # gradient is X^T(XR - Y) * 2/m\n",
        "    gradient = np.dot(X.transpose(),np.dot(X,R)-Y)*(2/m)\n",
        "    return gradient"
      ],
      "metadata": {
        "id": "OcECUzv2Ov8F"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "G5GWnbgH8Msu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training(X, Y, train_steps=100, learning_rate=0.0003):\n",
        "    \n",
        "    np.random.seed(129)\n",
        "\n",
        "    # R is a square matrix with length equal to the number of dimensions in th  word embedding\n",
        "    R = np.random.rand(X.shape[1], X.shape[1])\n",
        "\n",
        "    for i in range(train_steps):\n",
        "        if i % 25 == 0:\n",
        "            print(f\"loss at iteration {i} is: {compute_loss(X, Y, R):.4f}\")\n",
        "        gradient = compute_gradient(X,Y,R)\n",
        "\n",
        "        # update R by subtracting the learning rate times gradient\n",
        "        R -= learning_rate * gradient\n",
        "    return R"
      ],
      "metadata": {
        "id": "A47hTbowO03K"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "R_train = training(X_train, Y_train, train_steps=400, learning_rate=0.8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwZPafeePDtU",
        "outputId": "c45ff4b5-a10f-4ea1-e7e1-5f5cfd40b9cb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss at iteration 0 is: 963.0146\n",
            "loss at iteration 25 is: 97.8292\n",
            "loss at iteration 50 is: 26.8329\n",
            "loss at iteration 75 is: 9.7893\n",
            "loss at iteration 100 is: 4.3776\n",
            "loss at iteration 125 is: 2.3281\n",
            "loss at iteration 150 is: 1.4480\n",
            "loss at iteration 175 is: 1.0338\n",
            "loss at iteration 200 is: 0.8251\n",
            "loss at iteration 225 is: 0.7145\n",
            "loss at iteration 250 is: 0.6534\n",
            "loss at iteration 275 is: 0.6185\n",
            "loss at iteration 300 is: 0.5981\n",
            "loss at iteration 325 is: 0.5858\n",
            "loss at iteration 350 is: 0.5782\n",
            "loss at iteration 375 is: 0.5735\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applying K-nearst neighbours to get the corresponding french embedding"
      ],
      "metadata": {
        "id": "eI3kBTpb8Z_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nearest_neighbor(v, candidates, k=1):\n",
        "    \n",
        "    similarity_l = []\n",
        "\n",
        "    # for each candidate vector...\n",
        "    for row in candidates:\n",
        "        # get the cosine similarity\n",
        "        cos_similarity = cosine_similarity(v,row)\n",
        "\n",
        "        # append the similarity to the list\n",
        "        similarity_l.append(cos_similarity)\n",
        "        \n",
        "    # sort the similarity list and get the indices of the sorted list\n",
        "    sorted_ids = np.argsort(similarity_l)\n",
        "\n",
        "    # get the indices of the k most similar candidate vectors\n",
        "    k_idx = sorted_ids[-k:]\n",
        "    return k_idx"
      ],
      "metadata": {
        "id": "zITil5nLPLaA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(X, Y, R):\n",
        "    \n",
        "    # The prediction is X times R\n",
        "    pred = np.dot(X,R)\n",
        "\n",
        "    # initialize the number correct to zero\n",
        "    num_correct = 0\n",
        "\n",
        "    # loop through each row in pred (each transformed embedding)\n",
        "    for i in range(len(pred)):\n",
        "        # get the index of the nearest neighbor of pred at row 'i'; also pass in the candidates in Y\n",
        "        pred_idx = nearest_neighbor(pred[i],Y)\n",
        "\n",
        "        # if the index of the nearest neighbor equals the row of i... \\\n",
        "        if pred_idx == i:\n",
        "            # increment the number correct by 1.\n",
        "            num_correct += 1\n",
        "\n",
        "    # accuracy is the number correct divided by the number of rows in 'pred' (also number of rows in X)\n",
        "    accuracy = num_correct / len(pred)\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "POXSyd7YPZF1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val, Y_val = get_matrices(en_fr_test, fr_embeddings_subset, en_embeddings_subset)\n",
        "acc = compute_accuracy(X_val, Y_val, R_train)\n",
        "print(f\"accuracy on test set is {acc:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoYYw34vPlJc",
        "outputId": "f97b4d8d-df85-4b33-f355-e4122e2981f1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on test set is 0.557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p_-qYidzrekL"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}